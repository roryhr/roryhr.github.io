---
layout: post
title:  "Residual Networks"
date:   2016-05-11
categories: neural networks
---
# Motivation 

They say a picture is worth a thousand words. 
Images contain a lot of information and historically it's been impossible 
to understand even the simplest things about images. 


Companies like Facebook, Google, and Microsoft are racing to develop 
algorithms to monitize the data in images. 
Applications include adversising, the analysis of medical images, and self-driving cars.

# Technical Remix of He's Presentation

The Microsoft Research team of Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun 
won the 2015 ImageNet competition with their Residual Network. They achieved 
a 3.6% error rate across 1000 classes. 

Anticipating their 152 layer winning network (the deepest to date), He et al. noticed each winning network 
was deeper than the year before. 
This raises the question posed in the paper, "Is learning better networks as simple as stacking 
more layers?"

![ImageNet Presentation Slide 6](/images/ilsvrc2015_he-006.jpg)
![ImageNet Presentation Slide 3](/images/ilsvrc2015_he-003.jpg)


The answer is "yes" and "no". 
He showed  that adding traditional layers made a model perform worse (no). 
However, with a tweaked architecture adding more layers improves performance
and the “deeper is better” paradigm is preserved (yes).

![ImageNet Presentation Slide 22](/images/ilsvrc2015_he-022.jpg)

A basic residual block consists of two convolution layers with a shortcut connection 
across both layers.
Let the two weight layers constitute the function F(x) and 
the desired function be H(x). 
For a plain network, the weights learn H(x). 
In a residual network the weights learn H(x)-x. 

![ImageNet Presentation Slide 17](/images/ilsvrc2015_he-017.jpg)

# Not too deep enough

If deeper models are better, why stop at 152 layers?
The authors tried an "agressively deep" model with 1202 layers 
but it's error was higher. 

# Code

If you want to see some code... T 

### Original Recipe, Caffe

[Deep Residual Networks](https://github.com/KaimingHe/deep-residual-networks )

### Facebook, Torch 

[ResNet training in Torch](https://github.com/facebook/fb.resnet.torch)

### Yours Truly, Keras

Works with Theano or TensorFlow as the backend. 

[Keras ResNet](https://github.com/roryhr/keras_resnet)

