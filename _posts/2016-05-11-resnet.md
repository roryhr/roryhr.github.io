---
layout: post
title:  "ResNet"
date:   2016-05-11
categories: neural networks
---
# Convolutional Neural Networks

Great, but that raises the question: How do you get these probabilities?
This, of course, is the $1M dollar question but the answer is basically:
simulate the play for a large number of games.

# Residual Networks




# CIFAR10 ResNet for Keras

`resnet_utils.py`

~~~python
from __future__ import division
from keras.layers.convolutional import AveragePooling2D
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Dense, Activation, Flatten, Reshape, Lambda
from keras.layers import merge, Input
from keras.layers.normalization import BatchNormalization
from keras.models import Model
from keras.optimizers import SGD
from keras.regularizers import l2
from keras import backend as K


WEIGHT_DECAY = 0.0001
SHORTCUT_OPTION = 'A'


def zeropad(x):
    y = K.zeros_like(x)
    return K.concatenate([x, y], axis=1)


def zeropad_output_shape(input_shape):
    shape = list(input_shape)
    assert len(shape) == 4  # only valid for 4D tensors
    shape[1] *= 2
    return tuple(shape)


def base_convolution(input, nb_filters, conv_shape=(3, 3), stride=(1, 1),
                     relu_activation=True, **kwargs):
    """Convolution2D -> BatchNormalization -> ReLU"""

    x = Convolution2D(nb_filter=nb_filters,
                      nb_row=conv_shape[0], nb_col=conv_shape[1],
                      W_regularizer=l2(WEIGHT_DECAY),
                      subsample=stride,
                      border_mode='same',
                      init='he_normal',
                      **kwargs)(input)

    x = BatchNormalization()(x)
    if relu_activation:
        x = Activation('relu')(x)

    return x


def shortcut(input_layer, nb_filters, output_shape=None,
             upsample_method=SHORTCUT_OPTION):
    """Used to increase dimensions, ie 16 filters to 32 filters.

    Parameters
    ----------
    upsample_method : Bool  (for now)

        A: identity shortcut with zero-padding for increasing dimensions. 
           This is used for all CIFAR-10 experiments.
        B: identity shortcut with 1x1 convolutions for increasing dimensions. 
           This is used for most ImageNet experiments.
        C: 1x1 convolutions for all shortcut connections.
    """
    if upsample_method == 'A':
        # TODO: Figure out why zeros_upsample doesn't work in Theano
        # Option A: pad with zeros
        x = MaxPooling2D(pool_size=(1,1),
                         strides=(2,2),
                         border_mode='same')(input_layer)
        x = Lambda(zeropad, output_shape=zeropad_output_shape)(x)
    elif upsample_method == 'B':
        # B: pad with zeros
        x = Convolution2D(nb_filter=nb_filters, nb_col=1,nb_row=1,
                          subsample=(2,2),
                          border_mode='same')(input_layer)
    else:
        # My style: Take a 1x1 convolution over entire image with 1/4 the
        # filters and reshapes to the desired output shape with 2x the
        # number of filters. Works with old versions of TensorFlow where
        # the stride cannot be larger than the filter size.
        x = Convolution2D(nb_filter=nb_filters,
                          nb_row=1, nb_col=1,
                          W_regularizer=l2(WEIGHT_DECAY),
                          border_mode='same')(input_layer)
        x = BatchNormalization()(x)
    return x


def basic_block(input_layer, nb_filters, first_stride=(1, 1)):
    """Add a residual building block

    A residual block consists of 2 base convolutions with a short/identity
    connection between the input and output activation
    """

    # First convolution
    x = base_convolution(input=input_layer, nb_filters=nb_filters,
                         stride=first_stride)
    output_shape = x._keras_shape

    # Second Convolution, with Batch Normalization, without ReLU activation
    x = base_convolution(input=x, nb_filters=nb_filters, stride=(1, 1),
                         relu_activation=False)

    # Add the short convolution, with Batch Normalization
    if first_stride == (2, 2):
        input_layer = shortcut(input_layer, nb_filters, output_shape)

    x = merge(inputs=[x, input_layer], mode='sum')
    x = Activation('relu')(x)

    return x


def stack_units(input, block_unit, nb_blocks, nb_filters, stride=(1, 1)):
    x = block_unit(input=input, nb_filters=nb_filters,
                   first_stride=stride)

    for _ in range(nb_blocks-1):
        x = block_unit(input=x, nb_filters=nb_filters)

    return x
~~~

~~~ python
from __future__ import division
from keras.layers.convolutional import AveragePooling2D
from keras.layers.core import Dense, Flatten
from keras.layers import Input
from keras.models import Model
from keras.optimizers import SGD
from keras.datasets import cifar10
from keras.utils import np_utils

from resnet_utils import base_convolution, basic_block, stack_units


def build_residual_network(nb_blocks=[1, 3, 3, 3], input_shape=(3, 32, 32),
                           initial_nb_filters=16,
                           first_conv_shape=(3, 3)):
    """Construct a residual network model for CIFAR10.

    Parameters
    ----------
    nb_blocks : list
       The number of residual blocks for each layer group. For the 18-layer
       model nb_blocks=[1,2,2,2,2] and 34-layer nb_blocks=[1,3,4,6,3].
    initial_nb_filters : int, optional
       The initial number of filters to use. The number of filters is doubled
       for each layer.
    first_conv_shape : tuple of 2 ints
       The shape of the first convolution, also known as the kernel size.

    Returns
    -------
    input_image, output : input tensor, output class probabilities


    From the paper: input image 32x32 RGB image
    layer name      output size
    conv1           32x32           3x3, 16, stride 1
    conv2_x         32x32           3x3, 32, stride 2
    conv3_x         16x16           3x3, 32, stride 2
    conv4_x         8x8             8x8, average pool
    pool            1x1             10-d fc, softmax

    Reference: http://arxiv.org/abs/1512.03385
    """

    # -------------------------- Layer Group 1 ----------------------------
    input_image = Input(shape=input_shape)
    x = base_convolution(input=input_image, nb_filters=initial_nb_filters,
                         conv_shape=first_conv_shape)
    # Output size = 32x32
    # -------------------------- Layer Group 2 ----------------------------
    x = stack_units(input=x, block_unit=basic_block, nb_blocks=nb_blocks[1],
                    nb_filters=initial_nb_filters)
    # Output size = 32x32
    # -------------------------- Layer Group 3 ----------------------------
    x = stack_units(input=x, block_unit=basic_block, nb_blocks=nb_blocks[1],
                    nb_filters=initial_nb_filters * 2,
                    stride=(2, 2))
    # Output size = 16x16
    # -------------------------- Layer Group 4 ----------------------------
    x = stack_units(input=x, block_unit=basic_block, nb_blocks=nb_blocks[1],
                    nb_filters=initial_nb_filters * 4,
                    stride=(2, 2))
    # Output size = 8x8

    pool_size = x._keras_shape[-2:]
    x = AveragePooling2D(pool_size=tuple(pool_size))(x)
    # Output size = 1x1
    x = Flatten()(x)
    output = Dense(10, activation='softmax')(x)

    return input_image, output


if __name__ == '__main__':
    nb_classes = 10
    input_tensor, output_tensor = build_residual_network(initial_nb_filters=16,
                                                         nb_blocks=[1, 5, 5, 5],
                                                         first_conv_shape=(3,3),
                                                         input_shape=(3,32,32))

    model = Model(input=input_tensor, output=output_tensor)
    sgd = SGD(lr=0.1, decay=1e-4, momentum=0.9)
    model.compile(optimizer=sgd, loss='categorical_crossentropy',
                  metrics=['accuracy'])

    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')

    # convert class vectors to binary class matrices
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)

    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255

    # history = model.fit(X_train, y_train,
    #                     validation_data=(X_test, y_test),
    #                     batch_size=128)

    model.summary()
~~~