---
layout: post
title:  "ResNet"
date:   2016-05-11
categories: neural networks
---
# Motivation 

We could do a better job of selling ads if we knew what was in all these 
images people post to the internet. 

As a first step, tell us what's in an image. 

(As a bonus, the solution helps doctors analyze medical images,
expands access for the blind... We live in a visual world!)

# Technical Remix of He's Presentation

Anticipating their 152 layer winning network, He noticed the each winning network 
was deeper than the year before. 
This raises the question posed in the paper, "Is learning better networks as simple as stacking 
more layers?"

![ImageNet Presentation Slide 6](/images/ilsvrc2015_he-006.jpg)
![ImageNet Presentation Slide 3](/images/ilsvrc2015_he-003.jpg)


The answer is "yes" and "no". 
He showed  that adding traditional layers made a model perform worse (no). 
However, with a tweaked architecture adding more layers improves performance
and the “deeper is better” paradigm is preserved (yes).

![ImageNet Presentation Slide 22](/images/ilsvrc2015_he-022.jpg)

A residual block is simple consists of two convolution layers with a shortcut connection 
across both layers.
Let the two weight layers constitute the function F(x) and 
the desired function be H(x). 
For a plain network, the weights learn H(x). 
In a residual network the weights learn H(x)-x. 

![ImageNet Presentation Slide 17](/images/ilsvrc2015_he-017.jpg)



# Code

If you want to see some code... 

### Original Recipe, Caffe

[Deep Residual Networks](https://github.com/KaimingHe/deep-residual-networks )

### Facebook, Torch 

[ResNet training in Torch](https://github.com/facebook/fb.resnet.torch)

### Yours Truly, Keras

Works with Theano or TensorFlow as the backend. 

[Keras ResNet](https://github.com/roryhr/keras_resnet)

