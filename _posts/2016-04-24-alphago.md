---
layout: post
title:  "AlphaGo"
date:   2016-04-23 20:00:59 -0800
categories: reinforcement learning
---

[DeepMind videos](https://www.youtube.com/channel/UCP7jMXSY2xbc3KCAE0MHQ-A/videos)

[Atlantic](http://www.theatlantic.com/technology/archive/2016/04/how-alphago-imitates-human-intuition/476508/)

# How AlphaGo Works

There are tons of AlphaGo explainers...so here's another one. 

Suppose you didn't know anything about Poker but wanted to play anyway. 
All you'd need to know is your chance of winning at every stage of the game. 
You'd evaluate possible actions and choose the ones with the highest probability of winning. 

![Poker image](https://i.ytimg.com/vi/85s1tlX5iB0/maxresdefault.jpg)

Question: How do you get these probabilities?

Answer: By simulating a large number of Go games.

# How AlphaGo Works in too much detail

The paradigm for AlphaGo isn't deep learning, it's Monte Carlo Tree Search (MCTS). 

MCTS is a smart way of gathering statistics for games of perfect information where 
you're playing against an opponent. I'll refer you to other resources since I 
don't know more than that. 

[Monte Carlo Tree Search](http://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/)


![Fig. 3 from their Nature Paper](http://www.nature.com/nature/journal/v529/n7587/carousel/nature16961-f3.jpg)


Interpreting Fig. 3 (black to move)

a. Select the move with the maximum action-value Q plus an exploration term and repeat.

b. If a position hasn't been previously explored, it's time to evaluate possible moves with the policy network.

c. Moves are evaluated with the fast rollout policy and the value network

d. Action-values are backpropagated up the tree

The policy network limits the search space while the value network 
and fast rollout policy approximate rollouts to the end of the game.

# Technical Questions

1. Why don't they use the Q value to choose a move? N visits is more stable â€“ why?
 
2. How can AlphaGo be more efficient?

3. Why is the SL Policy used in liu of the RL-trained policy? It makes better, more diverse move selections. 
In other words why is the RL policy myopic?

4. Why didn't DQN work? Or rather, why couldn't it be made stronger without MCTS?
