<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Rory's Blog - neural networks</title>
        <link rel="stylesheet" href="../theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">Rory's Blog </a></h1>
                <nav><ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/about">About</a></li>
                    <li><a href="/blog/">Blog</a></li>
                    <li><a href="/projects">Projects</a></li>
                    <li><a href="/contact">Contact</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="../residual-networks.html">Residual Networks</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-05-11T00:00:00-07:00">
                Published: Wed 11 May 2016
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="../author/rory-hartong-redden.html">Rory Hartong-Redden</a>
        </address>
<p>In <a href="../category/blog.html">blog</a>.</p>
<p>tags: <a href="../tag/neural-networks.html">neural networks</a> </p>
</footer><!-- /.post-info --><h1>Abstract</h1>
<p>Understanding the high-level content in an image is a long-standing challenge in computer vision.
In the last five years, the state-of-the art has advanced far beyond scanning checks and facial recognition to the point where on certain benchmarks object classification performed by a machine exceeds human level accuracy. </p>
<p>A representative benchmark is the "ImageNet Large Scale Visual Recognition Challenge" which provides 1.2 million training images for identifying 1000 classes of objects. 
Companies like Facebook, Google, and Microsoft use this challenge as a proving ground for technology that will monetize images. 
In 2014 Google won the ImageNet competition with their "Inception" network and an accuracy of 6.7%. 
The winning network in 2015 was Microsoft Research's "ResNet" with an accuracy of 3.6%.</p>
<p>The winning architecture is notable for a few reasons. 
First, it commands attention simply by it's performance and striking improvement over last year's result. 
Second, the improvement was achieved using a reformulated network structure rather than via additional computation. 
Third, their residual theory is an attractively simple and extensible, paving the way for future improvements. </p>
<h1>Technical Summary of He's Presentation</h1>
<p>The Microsoft Research team of Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun 
won the 2015 ImageNet competition in a number of categories. 
In the object detection task an ensemble of residual models achieved a 3.6% classification error rate.</p>
<h2>Deeper Models</h2>
<p>Anticipating their 152 layer winning network (the deepest to date), He et al. noticed each winning network 
was deeper than the year before. 
This raises the question they pose, "Is learning better networks as simple as stacking more layers?"</p>
<p><img alt="ImageNet Presentation Slide 6" src="../images/ilsvrc2015_he-006.jpg" />
<img alt="ImageNet Presentation Slide 3" src="/images/ilsvrc2015_he-003.jpg" /></p>
<p>The answer is "yes" and "no". 
He showed  that adding traditional layers made a model perform worse (no). 
On the left side of the figure below, we see <em>deeper</em> networks perform <em>worse</em>. 
This is counterintuitive because the extra 12 layers in the 56-layer network 
could learn the identity shortcut and thus perform as well as the 44-layer network. 
In practice, the deeper plain networks converge to worse local optima than shallower counterparts. </p>
<p>However, with a residual architecture adding layers improves performance (yes).
When the identity connection is baked into the network it that helps the deeper networks converge to better solutions. </p>
<p><img alt="ImageNet Presentation Slide 22" src="/images/ilsvrc2015_he-022.jpg" /></p>
<h2>Residual Units</h2>
<p>A basic residual unit consists of two convolution layers with a shortcut connection across both layers.
Let the two weight layers constitute the function F(x) and the desired function be H(x). </p>
<p>For a plain network the weights learn H(x). </p>
<p>For a residual network the weights learn H(x)-x. </p>
<p><img alt="ImageNet Presentation Slide 17" src="/images/ilsvrc2015_he-017.jpg" />
The network is learning a function with respect to the identity function. 
It's not clear why this works better than, say, initializing the weights in a plain network identity matrices. </p>
<h1>Not too deep enough</h1>
<p>If deeper models are better, why stop at 152 layers?
The authors tried an "aggressively deep" model with 1202 layers 
but its error was higher. </p>
<h1>Code</h1>
<p>If you want to see some code... T </p>
<h3>Original Recipe, Caffe</h3>
<p><a href="https://github.com/KaimingHe/deep-residual-networks">Deep Residual Networks</a></p>
<h3>Facebook, Torch</h3>
<p><a href="https://github.com/facebook/fb.resnet.torch">ResNet training in Torch</a></p>
<h3>Yours Truly, Keras</h3>
<p>Works with Theano or TensorFlow as the backend. </p>
<p><a href="https://github.com/roryhr/keras_resnet">Keras ResNet</a></p>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://googleresearch.blogspot.com">Google Research</a></li>
                            <li><a href="http://blog.kaggle.com">No Free Hunch</a></li>
                            <li><a href="http://blog.keras.io">Keras</a></li>
                            <li><a href="http://efavdb.com">EFAVDB</a></li>
                            <li><a href="https://research.facebook.com/blog/">Facebook</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://github.com/roryhr">github</a></li>
                            <li><a href="https://www.linkedin.com/in/rory-hartong-redden-18334356">linkedin</a></li>
                            <li><a href="https://twitter.com/rory_h_r">twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>